[![Python version](https://img.shields.io/badge/python-3.6%20|%203.7-blue.svg)](https://www.python.org/downloads/)
[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)

![](res/readme/intelligent_header.png?raw=true)

Upload files or folders (even with subfolders) to Amazon S3 in a totally automatized way taking advantage of:

- **Amazon S3 Multipart Upload**: The uploaded files are processed transparently in [parts](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html) improving the throughput and the quick recovery from any network issues.

- **Resilent Retry System**: Intelligent S3 Upload has been built to detect any error during the uploading process and to perform any retries whenever is necessary. 

- **User Friendly Interface**: Just check the demo to see with your own eyes how the upload process is performed.    

[**Checkout the demo**](https://is3u.alzina.io)

## Installation

#### Clone the repository

```
git clone https://github.com/polius/intelligent-s3-upload.git
```

#### Install the dependencies

```
python3 -m pip install boto3 --user
python3 -m pip install requests --user
```

## Setup

Before executing the **Intelligent S3 Upload**, modify the [credentials.json](https://github.com/polius/intelligent-s3-upload/blob/master/app/credentials.json) file.

```
{
    "aws_access_key_id": "",
    "aws_secret_access_key": "",
    "region_name": "",
    "bucket_name": "",
    "bucket_prefix": "",
    "storage_class": "",
    "skip_s3_existing_files": true,
    "server_side_encryption": true,
    "slack_url": ""
}
```

- **aws_access_key_id | aws_secret_access_key**: Credentials generated by [Amazon IAM](https://www.cloudberrylab.com/resources/blog/how-to-find-your-aws-access-key-id-and-secret-access-key/#how-to-retrieve-iam-access-keys).
- **region_name**: The [AWS Region Code](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions) where the bucket is located. 
- **bucket_name**: The bucket name created by Amazon S3.
- **bucket_path**: (Optional) The bucket path to store the uploaded objects.
- **storage_class**: The type of storage to use for the uploaded object. These are the possible values:

| **storage_class** |
| ------ |
| STANDARD |
| REDUCED_REDUNDANCY |
| STANDARD_IA |
| ONEZONE_IA |
| INTELLIGENT_TIERING |
| GLACIER |
| DEEP_ARCHIVE |
| OUTPOSTS |

- **skip_s3_existing_files**: Skip uploading objects if these already exists in S3. Possible values: [ true | false ]
- **server_side_encryption**: Enable Server-side encryption using the Amazon S3 key (SSE-S3). Possible values: [ true | false ]
- **slack_url**: (Optional) Enter a Webhook URL to send a message to Slack when a upload finishes.

## AWS Policy

To be able to run the script check that your policy meets the following requirements:

```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": [
                "s3:PutObject",
                "s3:ListBucketMultipartUploads",
                "s3:AbortMultipartUpload",
                "s3:ListBucket",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::yourbucket/*",
                "arn:aws:s3:::yourbucket"
            ]
        }
    ]
}
```

## Execution

```
python3 upload.py --path "{PATH}"
```

Replace the **{PATH}** string with the absolute file/folder path.

## License

This project is licensed under the MIT license. See the [LICENSE](./LICENSE) file for more info.
