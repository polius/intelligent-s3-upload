[![Python version](https://img.shields.io/badge/python-3.6%20|%203.7-blue.svg)](https://www.python.org/downloads/)
[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE)

![](res/readme/intelligent_header.png?raw=true)

Upload files or folders (even with subfolders) to Amazon S3 in a totally automatized way taking advantage of:

- **Amazon S3 Multipart Upload**: The uploaded files are processed transparently in [parts](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html) improving the throughput and the quick recovery from any network issues.

- **Resilent Retry System**: Intelligent S3 Upload has been built to detect any error during the uploading process and to perform any retries whenever is necessary. 

- **User Friendly Interface**: Just check the demo to see with your own eyes how the upload process is performed.    

[**Checkout the demo**](https://is3u.poliuscorp.com)

## Installation

#### Clone the repository

```
$ git clone https://github.com/polius/intelligent-s3-upload.git
```

#### Install the dependencies

```
$ pip3 install boto3 --user
```

## Setup

Before executing the **Intelligent S3 Upload**, modify the [credentials.json](https://github.com/polius/intelligent-s3-upload/blob/master/app/credentials.json) file.

```
{
    "aws_access_key_id": "",
    "aws_secret_access_key": "",
    "region_name": "",
    "bucket_name": "",
    "bucket_prefix": "",
    "storage_class": "",
    "skip_s3_existing_files": true
}
```

- **aws_access_key_id | aws_secret_access_key**: Credentials generated by [Amazon IAM](https://www.cloudberrylab.com/resources/blog/how-to-find-your-aws-access-key-id-and-secret-access-key/#how-to-retrieve-iam-access-keys).
- **region_name**: The [AWS Region Code](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions) where the bucket is located. 
- **bucket_name**: The bucket name created by Amazon S3.
- **bucket_path**: (Optional) The bucket path to store the uploaded objects.
- **storage_class**: The type of storage to use for the uploaded object. These are the possible values:

| **storage_class** |
| ------ |
| STANDARD |
| REDUCED_REDUNDANCY |
| STANDARD_IA |
| ONEZONE_IA |
| INTELLIGENT_TIERING |
| GLACIER |
| DEEP_ARCHIVE |

- **skip_s3_existing_files**: Skip uploading objects if these already exists in S3. Possible values: [ true | false ]

## Execution

```
$ python3 upload.py --path "{PATH}"
```

Replace the string **{PATH}** with the absolute file/folder path to upload to Amazon S3.

## License

This project is licensed under the MIT license. See the [LICENSE](./LICENSE) file for more info.